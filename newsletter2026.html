<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>BrainGPT.org Newsletter</title>
  <style>
    body {
      margin: 0; padding: 0;
      background: #f4f4f4;
      font-family: Georgia, serif;
      color: #1a1a1a;
    }
    .wrapper {
      max-width: 660px;
      margin: 40px auto;
      background: #ffffff;
      border-radius: 6px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
    }
    .header {
      background: #1a2b4a;
      padding: 36px 40px 28px;
      text-align: center;
    }
    .header h1 {
      margin: 0 0 6px;
      font-family: 'Helvetica Neue', Helvetica, sans-serif;
      font-size: 34px;
      font-weight: 700;
      color: #ffffff;
      letter-spacing: 2px;
    }
    .header p {
      margin: 0;
      font-family: 'Helvetica Neue', Helvetica, sans-serif;
      font-size: 13px;
      color: #8aaad4;
      letter-spacing: 1px;
      text-transform: uppercase;
    }
    .body {
      padding: 36px 40px;
    }
    .intro {
      font-size: 16px;
      line-height: 1.7;
      color: #333;
      margin-bottom: 32px;
      border-left: 3px solid #1a2b4a;
      padding-left: 16px;
    }
    h2 {
      font-family: 'Helvetica Neue', Helvetica, sans-serif;
      font-size: 11px;
      font-weight: 700;
      letter-spacing: 2px;
      text-transform: uppercase;
      color: #8aaad4;
      margin: 40px 0 4px;
    }
    h3 {
      font-family: Georgia, serif;
      font-size: 20px;
      font-weight: bold;
      color: #1a2b4a;
      margin: 0 0 12px;
      line-height: 1.35;
    }
    p {
      font-size: 15px;
      line-height: 1.75;
      color: #444;
      margin: 0 0 14px;
    }
    a {
      color: #2a6db5;
      text-decoration: none;
    }
    a:hover { text-decoration: underline; }
    .coverage-links {
      background: #f0f5fb;
      border-radius: 4px;
      padding: 14px 18px;
      margin: 14px 0 0;
      font-size: 14px;
      line-height: 2;
    }
    .coverage-links strong {
      display: block;
      font-family: 'Helvetica Neue', Helvetica, sans-serif;
      font-size: 11px;
      letter-spacing: 1px;
      text-transform: uppercase;
      color: #888;
      margin-bottom: 4px;
    }
    .divider {
      border: none;
      border-top: 1px solid #e8e8e8;
      margin: 36px 0;
    }
    .cta-box {
      background: #f9f7f2;
      border-left: 4px solid #c8a84b;
      border-radius: 0 4px 4px 0;
      padding: 20px 22px;
      margin-bottom: 28px;
    }
    .cta-box h3 {
      font-size: 18px;
      margin-bottom: 10px;
    }
    .cta-box p {
      font-size: 14px;
      margin-bottom: 8px;
    }
    .cta-box p:last-child { margin-bottom: 0; }
    .footer {
      background: #1a2b4a;
      padding: 24px 40px;
      text-align: center;
      font-family: 'Helvetica Neue', Helvetica, sans-serif;
      font-size: 12px;
      color: #8aaad4;
      line-height: 1.8;
    }
    .footer a { color: #8aaad4; }
  </style>
</head>
<body>
<div class="wrapper">

  <!-- HEADER -->
  <div class="header">
    <h1><a href="https://braingpt.org" style="color:#ffffff; text-decoration:none;">BrainGPT</a></h1>
    <p>Research Updates &amp; Opportunities Â· 2026</p>
  </div>

  <!-- BODY -->
  <div class="body">

    <p class="intro">
      It's been a while â€” and a lot has happened. This issue covers a burst of research that has emerged from the <a href="https://braingpt.org">BrainGPT</a> project, some exciting follow-up work, and two calls to action I'd love your help with.
    </p>

    <!-- NEWS ITEM 1 -->
    <h2>Research News</h2>
    <h3>LLMs Surpass Human Experts at Predicting Neuroscience Results</h3>
    <p>
      The headline finding from the <a href="https://braingpt.org">BrainGPT</a> project is now published in <em>Nature Human Behaviour</em>: large language models outperform neuroscience experts at predicting the outcomes of neuroscience studies. This result surprised many in the field and generated significant discussion about what it means for scientific expertise and the future of research.
    </p>
    <p>
      Thanks to everyone who contributed to making this work happen.
    </p>
    <p>
      On the heels of this, we completed follow-up work funded by Foresight showing that the results generalize robustly â€” they hold across different question formats and superficial phrasing variations, not just the original benchmark. This work isn't publicly shareable yet, but stay tuned.
    </p>
    <div class="coverage-links">
      <strong>Coverage &amp; Links</strong>
      ğŸ“„ <a href="https://doi.org/10.1038/s41562-024-02046-9">Paper â€” <em>Nature Human Behaviour</em></a><br/>
      ğŸ“Š <a href="https://www.nature.com/articles/s41562-024-02046-9/metrics">Press coverage metrics</a><br/>
      ğŸ¥ <a href="https://www.youtube.com/watch?v=Qgrl3JSWWDE">Video explainer by Sabine Hossenfelder</a><br/>
 ğŸ™ï¸ <a href="https://touchneurology.com/podcast/braingpt-advancing-neuroscientific-research-with-ai/">Neurology Today Podcast</a><br/>
      ğŸ“° <a href="https://www.nature.com/articles/s41593-024-01860-8">Commentary â€” <em>Nature Neuroscience</em></a><br/>
      ğŸ“° <a href="https://www.thetransmitter.org/neuroscientists-using-ai/how-neuroscientists-are-using-ai/">Feature â€” The Transmitter</a>
    </div>

    <hr class="divider"/>

    <!-- NEWS ITEM 2 -->
    <h3>Humanâ€“AI Teams Outperform Either Alone</h3>
    <p>
      A new paper in <em>Patterns</em> finds that even though LLMs beat human experts, humans still have a role to play in prediction. In follow-up work led by Felipe YaÃ±ez, we found that <strong>humanâ€“AI teams outperform both humans and machines working alone</strong> â€” because humans and LLMs make different errors. For the moment, humans have a role to play in prediction because they complement machines. Congratulations to Felipe and everyone involved in this excellent follow-up. 
    </p>
    <div class="coverage-links">
      <strong>Paper</strong>
      ğŸ“„ <a href="https://www.cell.com/patterns/fulltext/S2666-3899(25)00271-5">Patterns â€” Humanâ€“AI Teaming in Scientific Prediction</a>
    </div>

    <hr class="divider"/>

    <!-- NEWS ITEM 3 -->
    <h3>Reversed Text, Same Performance â€” and What That Tells Us</h3>
    <p>
      A third line of work probes something deeper about <em>how</em> LLMs work. We found that LLMs trained on neuroscience text written in reverse order perform just as well â€” sometimes better â€” than those trained on forward text. This is striking: reversed text is unreadable to humans, yet it's no obstacle for transformers.
    </p>
    <p>

  We then proved formally that models <em>should</em> perform identically regardless of whether they are trained and tested on forward, reversed, or any other permutation of text â€” because all permutations estimate the same underlying distribution of conditional probabilities. However, in practice models often <em>do</em> diverge in their estimates depending on text order. That divergence provides a useful signal because it indicates a model's probability estimates are inconsistent and its outputs untrustworthy.
</p>
<p>
  The broader lesson is that LLMs seize on statistical patterns in ways that are fundamentally alien to how human learners process language â€” which is precisely what makes them powerful, and what makes understanding their failure modes so important.
</p>
    <div class="coverage-links">
      <strong>Papers</strong>
      ğŸ“„ <a href="https://arxiv.org/abs/2411.11061">arXiv: Beyond Human-Like Processing </a><br/>
      ğŸ“„ <a href="https://arxiv.org/abs/2505.08739">arXiv: Theoretical proof of permutation equivalence</a>
    </div>

    <hr class="divider"/>

    <!-- CALLS TO ACTION -->
    <h2>Calls to Action</h2>

    <!-- CTA 1 -->
    <div class="cta-box">
      <h3>ğŸš€ Building a Company â€” Looking for a Co-Founder &amp; Funders</h3>
      <p>
        Working on <a href="https://braingpt.org">BrainGPT.org</a> â€” and seeing its reception â€” convinced me there is a genuine need for a platform that helps scientists, funders, policy makers, journalists, and the public make sense of the scientific literature. Xiaoliang "Ken" Luo and I have been exploring the idea. Indeed, I moved to Los Alamos National Laboratory (LANL) in part in hopes of pursuing this vision. I've concluded that a company is the right instrument to realize this vision and achieve real impact.
      </p>
      <p>
        I am building a team and seeking funding. I am seeking a technical co-founder â€” ideally someone who has founded or scaled an early-stage platform before, with the execution experience to turn a vision into something the world actually uses.
      </p>
      <p>
        <strong>If you're interested in joining the team</strong> in any capacity, please reach out and tell me which roles (e.g., COO, ML engineer/scientist, software developer, etc.) fit you best. All backgrounds are welcome.
      </p>
      <p>
        <strong>If you know of funders</strong> active in the AI for Science space â€” whether motivated by returns or by public benefit â€” please send them my way, or send me their details. 
      </p>
    </div>

    <!-- CTA 2 -->
    <div class="cta-box">
      <h3>ğŸ§  Seeking a Collaborator: Encodingâ€“Decoding Tradeoffs in the Brain</h3>
      <p>
        Since joining LANL, my bandwidth for neuroscience projects has been limited â€” but some projects are too promising to let go. One is an ongoing collaboration with Brett Roads examining how the brain trades off between encoding information efficiently and decoding it accurately. This builds on Brett's prior work on psychological embeddings and human similarity judgments.
      </p>
      <p>
        What we need is someone with hands-on expertise in fMRI data analysis, particularly building encoding models for (masked) cortical regions of interest. This is likely a <strong>first-author opportunity</strong> on what should be a high-impact publication.
      </p>
      <p>
        If you have the skills and interest, please get in touch.
      </p>
      <div style="font-size:13px; margin-top:10px; color:#666;">
        Related work: 
        <a href="https://www.sciencedirect.com/science/article/pii/S136466132400189X">Roads &amp; Love (2024)</a> Â· 
        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Roads_Enriching_ImageNet_With_Human_Similarity_Judgments_and_Psychological_Embeddings_CVPR_2021_paper.pdf">Roads &amp; Love, CVPR 2021</a>
      </div>
    </div>

  </div><!-- /body -->

  <!-- FOOTER -->
<!-- FOOTER -->
  <div class="footer">
    BrainGPT Newsletter Â· Bradley C. Love<br/>
    <a href="https://bradlove.org" style="color:#8aaad4;">bradlove.org</a> Â· <a href="mailto:bradley.c.love@gmail.com">Reply to get in touch</a> Â· <a href="$[LI:UNSUBSCRIBE]$" style="color:#8aaad4;">Unsubscribe</a>
  </div>
</div>
</body>
</html>